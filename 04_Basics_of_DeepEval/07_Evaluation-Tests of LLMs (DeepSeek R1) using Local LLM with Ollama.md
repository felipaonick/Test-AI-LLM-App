## ğŸ§ª Esecuzione dei Test DeepEval su Modello Locale (Ollama)

### ğŸ¯ Obiettivo della Lezione
- Sostituire lâ€™uso dellâ€™API OpenAI (GPT-4) con un **modello locale**.
- Eseguire test in **modalitÃ  completamente offline**.
- Azzerare i costi legati allâ€™inferenza via API.

---

## ğŸ› ï¸ Requisiti
- **Modello Ollama giÃ  scaricato**, es: `deepseek-r1:8b` (`deepseek-r1`).
- Ollama in esecuzione locale.
- DeepEval giÃ  installato.
- API ConfidentAI giÃ  configurata (opzionale, per visualizzazione in dashboard cloud).

---

## ğŸ”§ Impostazione del Modello Locale

Per dire a DeepEval di usare il modello Ollama locale:

```bash
!deepeval set-ollama-model deepseek-llm:8b-instruct
```

âœ… Risultato atteso:
```
Congratulations! You are now using local Ollama model for all the evaluation that requires an LM.
```

---

## ğŸ§ª Esecuzione del Test: `dataset.evaluate()`

Una volta impostato il modello locale, Ã¨ possibile eseguire la valutazione:

```python
dataset.evaluate(metrics=[AnswerRelevancyMetric()])
```

ğŸ” Il test verrÃ  eseguito utilizzando il **modello locale Ollama**, ad esempio `deepseek-r1`, invece di GPT-4.

### âœ… Output atteso
- Nessun costo di esecuzione.
- Risultati identici (se il modello Ã¨ accurato).
- Risposta elaborata in locale.
- Risultati visibili anche nel portale **Confident AI** se il login Ã¨ attivo.

---

## ğŸ“Š Confronto risultati

Nel portale **Confident AI**, sarÃ  possibile confrontare:
- Risultati ottenuti tramite OpenAI (GPT-4).
- Risultati ottenuti con modelli locali Ollama.

### ğŸŸ¢ Vantaggi:
| Aspetto             | Modello Locale Ollama        | API OpenAI GPT-4       |
|---------------------|------------------------------|-------------------------|
| ğŸ’¸ Costo            | Nessuno                      | SÃ¬ (a consumo)          |
| ğŸ” Privacy          | Totale (nessun invio dati)   | Dati inviati a OpenAI   |
| ğŸš€ VelocitÃ          | Dipende dal proprio hardware | Costante ma remoto      |
| ğŸ§ª RipetibilitÃ      | Alta (modello sempre uguale) | Possibili update invisibili |

---

## ğŸ’¡ Suggerimento Avanzato

Ãˆ possibile usare anche un comando alternativo con `base_url`:

```bash
!deepeval set-ollama-model deepseek-r1:8b-instruct --base-url http://localhost:11434
```

Questo Ã¨ utile se:
- Ollama gira su un container Docker.
- Si usa una porta diversa da quella predefinita.

---

## âœ… Conclusione

- Ora siamo in grado di **eseguire test DeepEval completamente offline**.
- Nessun costo ulteriore sarÃ  sostenuto.
- La qualitÃ  del test dipende dal **modello Ollama selezionato**.
- La metodologia rimane identica a quella usata con OpenAI.

---

## ğŸ“Prossimi Passi

Nelle prossime lezioni:
- Useremo questa configurazione con **metriche multiple**.
- Scopriremo come **eseguire test avanzati** con agenti, tool calling, e RAG.
- Pubblicheremo test anche **senza Confident AI**, salvando i risultati in file locali.
