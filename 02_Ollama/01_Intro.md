# üß† Eseguire un LLM in locale con Ollama ‚Äì Appunti Dettagliati

> Lezione del: 15/04/2025
---

## üöÄ Obiettivo della sezione

- Imparare a **eseguire localmente modelli LLM** (Large Language Models) usando [**Ollama**](https://ollama.com).
- Ridurre i costi evitando l‚Äôutilizzo di API commerciali come OpenAI, Claude o Gemini.
- Consentire test e sviluppo **offline e gratuiti**, direttamente su Mac, Linux o Windows.

---

## üåê Cos'√® Ollama?

- Ollama √® un'applicazione multipiattaforma (macOS, Linux, Windows) che permette di:
  - **Eseguire modelli LLM** direttamente sulla propria macchina.
  - Scaricare e utilizzare facilmente modelli open-source per chat, RAG, agenti, embedding, visione, ecc.
- Non √® necessaria alcuna registrazione o API key.
- Alternativa gratuita a soluzioni a pagamento come OpenAI API.

---

## üí∏ Confronto con le API commerciali

| **Servizio**         | **Metodo**         | **Costo stimato (GPT-4, Claude, Gemini)** |
|----------------------|--------------------|-------------------------------------------|
| OpenAI API           | cloud, a pagamento | ~30$/milione token input, 60$/milione output |
| Ollama               | locale, gratuito   | 0‚Ç¨, richiede solo spazio su disco e risorse locali |

---

## üñ•Ô∏è Installazione di Ollama

1. **Vai su** [ollama.com](https://ollama.com)
2. Clicca su **Download**
3. Seleziona il tuo sistema operativo:
   - macOS (supportato nativamente)
   - Linux
   - Windows (disponibile)
4. Installa l'applicazione seguendo la guida della piattaforma.

‚ö†Ô∏è **Nota**: √à raccomandato completare l'installazione **prima della prossima lezione**, dove useremo Ollama per test locali.

---

## üß† Selezione del modello

Dalla sezione *Models* del sito, Ollama offre l‚Äôaccesso ai modelli LLM pi√π popolari, come:

| **Modello**           | **Parametri** | **Tipo**           | **Note**                               |
|-----------------------|---------------|--------------------|----------------------------------------|
| `llama3:8b`, `llama3:70b` | 8B, 70B     | Chat               | Ottimizzati per risposte conversazionali |
| `mistral`              | 7B            | Chat / Agent       | Leggero e veloce                       |
| `qwen2.5`              | 14B           | Tool use           | Supporta tool usage                    |
| `deepseek-coder`       | 33B           | Codice             | Ottimizzato per completamento codice  |
| `phi-2`, `gemma`, ecc. | Vari          | Embedded / Vision  | Alcuni specifici per embedding o visione |

### ‚úÖ Selezione avanzata:
- √à possibile filtrare per:
  - üîß **Tool support**
  - üß† **Embedding**
  - üëÅÔ∏è **Vision**

---

## üí° Vantaggi dell‚Äôutilizzo locale

- **Risparmio economico**: nessun pagamento per token.
- **Velocit√† di esecuzione**: tutto avviene sulla macchina locale.
- **Flessibilit√†**: test rapidi di modelli diversi, incluse versioni personalizzate.
- **Privacy e sicurezza**: i dati non lasciano la macchina.

---

## üß™ Prossimi passi

- Installa Ollama sul tuo sistema operativo.
- Nella prossima lezione vedremo come:
  - Scaricare un modello specifico.
  - Interrogarlo da terminale o tramite script.
  - Integrarlo nella tua app AI per test locali.
