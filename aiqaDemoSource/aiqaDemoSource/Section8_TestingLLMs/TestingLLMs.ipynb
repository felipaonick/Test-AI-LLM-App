{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing LLMs\n",
    "\n",
    "In this section, we will discuss testing LLMs which we used as Judge to evaluate our metrics before in DeepEval and RAGAs\n",
    "\n",
    "<img src=\"./img/TestingLLMs.png\" width=\"800\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Code using Ollama for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product\n",
      "Sentiment: The sentiment of the text \"I love this product\" is:\n",
      "\n",
      "**POSITIVE**\n",
      "\n",
      "This statement expresses strong positive feelings towards the product in question.\n",
      "Text: I hate the food in this resturant\n",
      "Sentiment: The sentiment of the text \"I hate the food in this restaurant\" is:\n",
      "\n",
      "NEGATIVE\n",
      "\n",
      "This statement expresses a strong dislike or dissatisfaction with the food at the restaurant, which indicates a negative sentiment.\n",
      "Text: The movie was just okay\n",
      "Sentiment: The sentiment analysis for \"The movie was just okay\" would be:\n",
      "\n",
      "- **POSITIVE**: 0 (There's no positive connotation)\n",
      "- **NEGATIVE**: 0 (No negative words or connotations are present)\n",
      "- **NEUTRAL**: 1 (This phrase indicates a neutral stance, suggesting the movie met basic expectations but did not exceed them)\n",
      "\n",
      "The phrase \"just okay\" generally implies that the experience was neither particularly good nor bad, hence it is classified as neutral.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def analyse_sentiment(text):\n",
    "    prompt = f\"Analyse the sentiment of this text and give me POSITIVE, NEGATIVE and NEUTRAL for this {text}\"\n",
    "    \n",
    "    response = ollama.chat(model='qwen2.5:latest',messages=[{'role': 'user', 'content': prompt}])\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "texts = [\n",
    "    \"I love this product\",\n",
    "    \"I hate the food in this resturant\",\n",
    "    \"The movie was just okay\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    sentiment = analyse_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Summarization (Classification) üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "        Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as ‚Äútool use,‚Äù function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works. \n",
      "\n",
      "        Without MCP, when you use a function call directly with an LLM API, you need to:\n",
      "\n",
      "        Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.\n",
      "        Implement handlers (the actual code that executes when a function is called) for those functions.\n",
      "        Create different implementations for each model you support.\n",
      "        MCP standardizes this process by:\n",
      "\n",
      "        Defining a consistent way to specify tools (functions) across any AI system.\n",
      "        Providing a protocol for discovering available tools and executing them.\n",
      "        Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.\n",
      "        You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API call with it. While this allows some purpose-built tooling, it‚Äôs bound to OpenAI‚Äôs ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.\n",
      "\n",
      "Summary: Here's a simplified summary:\n",
      "\n",
      "Function calling is a feature of modern AI models that allows them to execute predetermined functions based on user requests. Without a standardized way to do this (MCP), developers had to create custom code for each model they supported.\n",
      "\n",
      "MCP standardizes the process by providing a consistent way to define and use tools (functions) across any AI system, allowing users to easily discover and execute available tools without custom integration code.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def summarize_text(text):\n",
    "    prompt = f\"Please provide a consice summary of the given {text} and keep it as simple as possible\"\n",
    "    \n",
    "    response = ollama.chat(model='llama3.2:latest',messages=[{'role': 'user', 'content': prompt}])\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "text = \"\"\"\n",
    "        Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as ‚Äútool use,‚Äù function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works. \n",
    "\n",
    "        Without MCP, when you use a function call directly with an LLM API, you need to:\n",
    "\n",
    "        Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.\n",
    "        Implement handlers (the actual code that executes when a function is called) for those functions.\n",
    "        Create different implementations for each model you support.\n",
    "        MCP standardizes this process by:\n",
    "\n",
    "        Defining a consistent way to specify tools (functions) across any AI system.\n",
    "        Providing a protocol for discovering available tools and executing them.\n",
    "        Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.\n",
    "        You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API call with it. While this allows some purpose-built tooling, it‚Äôs bound to OpenAI‚Äôs ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_text(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Library (Transformer) from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (76.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.9.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.13.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.14.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-macosx_12_0_arm64.whl (252.7 MB)\n",
      "Using cached absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.13.0-cp312-cp312-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached keras-3.9.1-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl (670 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.14.1-cp312-cp312-macosx_11_0_arm64.whl (339 kB)\n",
      "Installing collected packages: namex, libclang, werkzeug, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.2.1 astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 h5py-3.13.0 keras-3.9.1 libclang-18.1.1 markdown-3.7 ml-dtypes-0.5.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-2.5.0 werkzeug-3.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (76.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install evaluate\n",
    "# !pip install tensorflow\n",
    "# !pip install tf-keras\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9976416826248169}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "classifier(\"I think I love this food eventhough many people says its terrible and bad and worst\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Summarization Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743275381.779261 21121308 service.cc:152] XLA service 0x374771870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743275381.779322 21121308 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "2025-03-30 08:09:41.786351: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743275381.843120 21121308 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summary = summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function calling allows LLMs to invoke predetermined functions based on user requests . the new protocol simply standardizes how this API feature works . MCP brings similar capabilities to any AI application that implements the protocol .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing custom LLM model from HF üß†ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'sadness', 'score': 0.28763842582702637}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"ExecuteAutomation/bert-base-text-classification-model\")\n",
    "\n",
    "# classifier([\n",
    "#     'I like learning AI Testing',\n",
    "#     'I started annoyed with my laptop which has no GPU',\n",
    "#     'I am low today',\n",
    "#     'I think I love this food eventhough many people says its terrible and bad and worst'\n",
    "# ])\n",
    "\n",
    "classifier('My child plays tennis all the time, and this affects his studies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to FacebookAI/roberta-large-mnli and revision 2a8f12d (https://huggingface.co/FacebookAI/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I love learning Machine Learning and AI Testing from ExecuteAutomation',\n",
       " 'labels': ['education', 'motivation', 'business', 'marketing'],\n",
       " 'scores': [0.48328739404678345,\n",
       "  0.47531428933143616,\n",
       "  0.02796463668346405,\n",
       "  0.013433674350380898]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "classifier(\"I love learning Machine Learning and AI Testing from ExecuteAutomation\",\n",
    "           candidate_labels=[\"education\", \"marketing\", \"motivation\", \"business\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Device set to use 0\n",
      "/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9893281,\n",
       "  'word': 'Karthik',\n",
       "  'start': 0,\n",
       "  'end': 7},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9558758,\n",
       "  'word': 'ExecuteAutomation',\n",
       "  'start': 22,\n",
       "  'end': 39},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9997625,\n",
       "  'word': 'NZ',\n",
       "  'start': 54,\n",
       "  'end': 56},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.7651939,\n",
       "  'word': 'karthik',\n",
       "  'start': 73,\n",
       "  'end': 80}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "classifier(\"Karthik is working in ExecuteAutomation and living in NZ and my email is karthik@ea.com\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
