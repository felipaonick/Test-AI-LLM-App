{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications üìë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Application\n",
    "This application reads data about Model Context Protocol (MCP) server from internet, stores in vector stores, chunks the data with embedding and useful to answer the question about MCP while inferenced.\n",
    "\n",
    "<img src=\"./img/RAG.png\" width=\"500\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_15624/1491807655.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external services and platforms. It supports actions like retrieving channel history from messaging apps and performing GitHub operations such as creating forks or branches. MCP servers, including reference, official integrations, and community-developed ones, demonstrate its capabilities in integrating diverse systems.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is MCP\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Application with DeepEval\n",
    "<img src=\"./img/RAGTesting.png\" width=\"800\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/deepeval/__init__.py:54: UserWarning: You are using deepeval version 2.5.9, however version 2.6.3 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "  input=\"What is MCP\",\n",
    "  actual_output= chain.invoke(\"What is MCP\"),\n",
    "  expected_output=\"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    ")\n",
    "\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=[test_case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMTestCase(input='What is MCP', actual_output='MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps, creating forks or branches on GitHub, and more. MCP servers, including reference, official integrations, and community-developed ones, showcase how AI assistants can access and utilize information from different sources.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "concise_metrics = GEval(\n",
    "    name = \"Concise\",\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "completness_metrics = GEval(\n",
    "    name = \"Completeness\",\n",
    "    criteria=\"Assess whether the actual output retains all the key information from the input\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üéâü•≥ Congratulations! You've successfully logged in! üôå \n",
       "</pre>\n"
      ],
      "text/plain": [
       "üéâü•≥ Congratulations! You've successfully logged in! üôå \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"chf7LtTWtK1foTOAiK+vHFZ622I16kZtcpzfC+7FAVU=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/deepeval/__init__.py:54: UserWarning: You are using deepeval version 2.5.9, however version 2.6.3 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "üôå Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GEval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Completeness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mCompleteness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:50, 50.50s/test case]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Completeness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: All key components are present and accurately aligned with their sources. No omissions or distortions; context and meaning are retained., error: None)\n",
      "  - ‚úÖ Answer Relevancy (score: 0.8333333333333334, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.83 because the actual output included an irrelevant statement about GitHub branches, which does not address what MCP is., error: None)\n",
      "  - ‚úÖ Concise (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The content is concise and includes all essential information without redundancy. The language is clear and complete, ensuring nothing vital is omitted., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP\n",
      "  - actual output: MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps, creating forks or branches on GitHub, and more. MCP servers, including reference, official integrations, and community-developed ones, showcase how AI assistants can access and utilize information from different sources.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Completeness (GEval): 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Concise (GEval): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! View results on \n",
       "\u001b]8;id=204505;https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=204505;https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Completeness (GEval)', threshold=0.5, success=True, score=1.0, reason='All key components are present and accurately aligned with their sources. No omissions or distortions; context and meaning are retained.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\nAssess whether the actual output retains all the key information from the input \\n \\nEvaluation Steps:\\n[\\n    \"Compare the input and output to ensure all key components are present.\",\\n    \"Verify that each piece of information aligns accurately with its source.\",\\n    \"Check for any omissions or distortions in the representation of details.\",\\n    \"Assess whether the retained information maintains its original context and meaning.\"\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=0.8333333333333334, reason='The score is 0.83 because the actual output included an irrelevant statement about GitHub branches, which does not address what MCP is.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"MCP stands for Model Context Protocol.\",\\n    \"It is a protocol designed to enable AI assistants to interact with external APIs and platforms.\",\\n    \"MCP supports actions such as retrieving channel history from messaging apps.\",\\n    \"It also allows creating forks or branches on GitHub.\",\\n    \"MCP servers include reference, official integrations, and community-developed ones.\",\\n    \"These servers showcase how AI assistants can access and utilize information from different sources.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"MCP allowing GitHub branches is not relevant to the input which asks for what MCP is.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Concise (GEval)', threshold=0.5, success=True, score=1.0, reason='The content is concise and includes all essential information without redundancy. The language is clear and complete, ensuring nothing vital is omitted.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Check for conciseness by ensuring the content is not overly wordy or redundant.\",\\n    \"Verify that all essential information is included without omitting critical details.\",\\n    \"Assess clarity: language should be clear and easily understandable without ambiguity.\",\\n    \"Evaluate balance between brevity and completeness, ensuring nothing vital is omitted.\"\\n]')], conversational=False, multimodal=False, input='What is MCP', actual_output='MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps, creating forks or branches on GitHub, and more. MCP servers, including reference, official integrations, and community-developed ones, showcase how AI assistants can access and utilize information from different sources.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None)], confident_link='https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8hz5ov62ohsxnpholjw4iv5/test-cases')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "deepeval.evaluate(dataset, metrics=[\n",
    "    completness_metrics,\n",
    "    deepeval.metrics.AnswerRelevancyMetric(),\n",
    "    concise_metrics\n",
    "] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
