{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications - (Advanced ‚ö°Ô∏è) üìë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Application\n",
    "This application reads data about Model Context Protocol (MCP) server from internet, stores in vector stores, chunks the data with embedding and useful to answer the question about MCP while inferenced.\n",
    "\n",
    "<img src=\"./img/RAG.png\" width=\"500\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps and performing Git operations on GitHub. MCP servers, which include reference, official integrations, and community servers, demonstrate how different systems can integrate with this protocol to enhance their functionality with AI assistant capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is MCP\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Application with DeepEval\n",
    "<img src=\"./img/RAGTesting.png\" width=\"800\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üéâü•≥ Congratulations! You've successfully logged in! üôå \n",
       "</pre>\n"
      ],
      "text/plain": [
       "üéâü•≥ Congratulations! You've successfully logged in! üôå \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"chf7LtTWtK1foTOAiK+vHFZ622I16kZtcpzfC+7FAVU=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/deepeval/__init__.py:54: UserWarning: You are using deepeval version 2.5.9, however version 2.6.3 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "üôå Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"expected_output\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"expected_output\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"expected_output\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "goldens = []\n",
    "\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data['input'],\n",
    "        expected_output=data['expected_output']\n",
    "    )\n",
    "    \n",
    "    goldens.append(golden)\n",
    "    \n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='What is MCP', actual_output=None, expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None), Golden(input='What is Relationship between function calling & Model Context Protocol', actual_output=None, expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None), Golden(input='What are the core components of MCP, just give the heading', actual_output=None, expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None)], conversational_goldens=[], _alias=None, _id=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.pull(alias=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[LLMTestCase(input='What is MCP', actual_output=None, expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None), LLMTestCase(input='What is Relationship between function calling & Model Context Protocol', actual_output=None, expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None), LLMTestCase(input='What are the core components of MCP, just give the heading', actual_output=None, expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None)], goldens=[], conversational_goldens=[], _alias=test, _id=cm8dznxvr19cg7x4vfai8djkh)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# It is going to use the LLM and Vector database stored information (RAG)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_80821/3706383010.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain(\"What is MCP\")\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain(\"What is MCP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\n",
      "\n",
      "LangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\n",
      "\n",
      "What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\n",
      "\n",
      "LinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\n"
     ]
    }
   ],
   "source": [
    "# Is the data which is stored in Vector DB\n",
    "retrieved_document = retrieve_and_format(\"What is MCP\")\n",
    "print(retrieved_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_context(question):\n",
    "    retrieved_document = retrieve_and_format(question)\n",
    "    response = qa_chain.run(question)\n",
    "    return retrieved_document, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_80821/2296904857.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\\n\\nLinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\",\n",
       " 'MCP stands for Model Context Protocol. It is a protocol designed to facilitate interactions between AI assistants and various external services or platforms. This includes support for actions like retrieving channel history from messaging apps, creating forks or branches on GitHub, listing issues, making pull requests, and searching code across GitHub repositories. MCP also encompasses a variety of servers that demonstrate how the protocol can be implemented, including reference servers created by protocol maintainers, official integrations maintained by companies, and community-developed servers. The protocol is versatile and can potentially retrieve information from diverse sources, showcasing its adaptability in different contexts.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual, context = query_with_context(\"What is MCP\")\n",
    "\n",
    "actual, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LLMTestCase with Goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        context, rag_response = query_with_context(golden.input)\n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=rag_response,\n",
    "            expected_output=golden.expected_output,\n",
    "            retrieval_context=[context],\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMTestCase(input='What is MCP', actual_output=\"MCP stands for Model Context Protocol. It's a protocol designed to enable AI assistants to interact with external APIs and retrieve information from various sources, including popular messaging apps and GitHub repositories. The protocol supports a wide range of actions such as creating forks or branches, listing issues, making pull requests, and searching for code across GitHub repositories. MCP servers can be reference, official integrations by companies, or community-developed, demonstrating its versatility within the AI assistant ecosystem.\", expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=[\"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\\n\\nLinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\"], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='What is Relationship between function calling & Model Context Protocol', actual_output=\"The relationship between function calling and the Model Context Protocol (MCP) lies in their roles within AI application development, particularly when it comes to integrating external APIs.\\n\\nFunction calling allows large language models (LLMs) to invoke predetermined functions based on user requests. This is a well-established feature that facilitates the execution of specific tasks or operations as directed by the user's input.\\n\\nThe Model Context Protocol builds upon this concept by providing a more structured and standardized way for AI applications to access context and interact with external APIs. It aims to simplify and standardize API interaction, making development simpler and more consistent across different applications regardless of the underlying model vendor.\\n\\nIn essence, while function calling enables LLMs to perform specific actions based on user input, MCP enhances this by providing a framework that allows AI apps to understand and utilize context more effectively. MCP can be seen as an extension or improvement over traditional function calling methods, especially when it comes to dealing with the complexity of integrating multiple APIs and managing context across different applications.\\n\\nFor example, MCP can help in retrieving information from various sources (like messaging apps or GitHub repositories) by standardizing how these interactions are handled, thus providing a more seamless experience for developers building AI applications.\", expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=['then make the API call with it. While this allows some purpose-built tooling, it‚Äôs bound to OpenAI‚Äôs ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools. Similarly, the aim of\\n\\nreactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nunderstand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP'], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='What are the core components of MCP, just give the heading', actual_output='MCP Client & Server Ecosystem', expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=['understand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\nsupports MCP, several processes occur behind the scenes to enable quick and seamless communication between the AI and external systems. Let‚Äôs take a closer look at what happens when a user asks Claude Desktop to perform a task that invokes tools outside the chat window.Protocol handshakeInitial connection: When an MCP client (like Claude Desktop) starts up, it connects to the configured MCP servers on your device.Capability discovery: The client asks each server \"What capabilities do you\\n\\nproblemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But'], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (3/3) [Time Taken: 02:33, 51.11s/test case] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because the answer directly addresses the question by listing the core components with appropriate headings., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because there are no contradictions in the context, meaning the actual output aligns perfectly with the retrieval context., error: None)\n",
      "  - ‚úÖ Contextual Precision (score: 0.9166666666666666, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.92 because the retrieval contexts provided include four relevant nodes that directly address the core components of MCP with clear explanations, while one node does not contribute to the topic. Although all 'yes' verdicts offer valuable information, their reasons are slightly less detailed in some cases., error: None)\n",
      "  - ‚ùå Contextual Relevancy (score: 0.375, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.38 because the retrieval context discusses technical details unrelated to MCP's core components, such as Docker, HubSpot, and security aspects, while the relevant statements focus on function calling in AI models., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are the core components of MCP, just give the heading\n",
      "  - actual output: MCP Client & Server Ecosystem\n",
      "  - expected output:  \n",
      "                    - MCP Client\n",
      "                    - MCP Servers\n",
      "                    - Protocol Handshake\n",
      "                    - Capability Discovery\n",
      "                \n",
      "  - context: None\n",
      "  - retrieval context: ['understand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\nsupports MCP, several processes occur behind the scenes to enable quick and seamless communication between the AI and external systems. Let‚Äôs take a closer look at what happens when a user asks Claude Desktop to perform a task that invokes tools outside the chat window.Protocol handshakeInitial connection: When an MCP client (like Claude Desktop) starts up, it connects to the configured MCP servers on your device.Capability discovery: The client asks each server \"What capabilities do you\\n\\nproblemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 0.8, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.80 because the actual output contained an irrelevant statement that does not address the question about MCP., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because there are no contradictions found between the actual output and the retrieval context., error: None)\n",
      "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because all relevant nodes are ranked higher than irrelevant ones., error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 0.5714285714285714, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.57 because the retrieval context is too generic and does not provide specific information about what MCP is., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP\n",
      "  - actual output: MCP stands for Model Context Protocol. It's a protocol designed to enable AI assistants to interact with external APIs and retrieve information from various sources, including popular messaging apps and GitHub repositories. The protocol supports a wide range of actions such as creating forks or branches, listing issues, making pull requests, and searching for code across GitHub repositories. MCP servers can be reference, official integrations by companies, or community-developed, demonstrating its versatility within the AI assistant ecosystem.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: [\"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\\n\\nLinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because the provided answer did not address the relationship between function calling and Model Context Protocol, focusing instead on unrelated topics like the importance of understanding relationships in general., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because there are no contradictions found between the actual output and the retrieval context., error: None)\n",
      "  - ‚úÖ Contextual Precision (score: 0.9266666666666665, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.93 because despite several relevant nodes ranking higher (nodes 1-5), one node (node 6) fails to discuss the relationship directly, making some relevance missing., error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 0.8888888888888888, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.89 because the retrieval context provides specific details about the Model Context Protocol, including its architecture and capabilities related to function calling and integration with various tools, which are relevant to understanding the relationship between function calling and MCP., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Relationship between function calling & Model Context Protocol\n",
      "  - actual output: The relationship between function calling and the Model Context Protocol (MCP) lies in their roles within AI application development, particularly when it comes to integrating external APIs.\n",
      "\n",
      "Function calling allows large language models (LLMs) to invoke predetermined functions based on user requests. This is a well-established feature that facilitates the execution of specific tasks or operations as directed by the user's input.\n",
      "\n",
      "The Model Context Protocol builds upon this concept by providing a more structured and standardized way for AI applications to access context and interact with external APIs. It aims to simplify and standardize API interaction, making development simpler and more consistent across different applications regardless of the underlying model vendor.\n",
      "\n",
      "In essence, while function calling enables LLMs to perform specific actions based on user input, MCP enhances this by providing a framework that allows AI apps to understand and utilize context more effectively. MCP can be seen as an extension or improvement over traditional function calling methods, especially when it comes to dealing with the complexity of integrating multiple APIs and managing context across different applications.\n",
      "\n",
      "For example, MCP can help in retrieving information from various sources (like messaging apps or GitHub repositories) by standardizing how these interactions are handled, thus providing a more seamless experience for developers building AI applications.\n",
      "  - expected output: The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\n",
      "  - context: None\n",
      "  - retrieval context: ['then make the API call with it. While this allows some purpose-built tooling, it‚Äôs bound to OpenAI‚Äôs ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools. Similarly, the aim of\\n\\nreactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nunderstand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Precision: 100.00% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! View results on \n",
       "\u001b]8;id=331994;https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=331994;https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742528120.407456 6411327 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_2', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer directly addresses the question by listing the core components with appropriate headings.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"The MCP ecosystem includes both client and server components.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions in the context, meaning the actual output aligns perfectly with the retrieval context.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP connects AI apps to context while building on top of function calling.\"\\n] \\n \\nClaims:\\n[\\n    \"The MCP ecosystem consists of both client and server components.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=0.9166666666666666, reason=\"The score is 0.92 because the retrieval contexts provided include four relevant nodes that directly address the core components of MCP with clear explanations, while one node does not contribute to the topic. Although all 'yes' verdicts offer valuable information, their reasons are slightly less detailed in some cases.\", strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context directly lists the core components of MCP as \\'MCP Client\\', \\'MCP Servers\\', \\'Protocol Handshake\\', \\'Capability Discovery\\'.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"It explains how MCP clients and servers interact through a protocol handshake to enable communication between AI tools and external systems.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context does not mention the NxM problem in relation to MCP, so it\\'s irrelevant to this question.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"It provides details about security considerations for MCP servers using OAuth and SSE transport, which are part of the protocol handshake process.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.375, reason=\"The score is 0.38 because the retrieval context discusses technical details unrelated to MCP's core components, such as Docker, HubSpot, and security aspects, while the relevant statements focus on function calling in AI models.\", strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"understand that MCP doesn\\\\u2019t solve the NxM problem by simply replacing the integration methods that came before\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"It connects AI apps to context while building on top of function calling\\\\u2014the primary method for calling APIs from LLMs\\\\u2014to make development simpler and more consistent\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Sometimes tinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement mentions \\'Docker,\\' which is not related to the core components of MCP as discussed in the context.\"\\n            },\\n            {\\n                \"statement\": \"HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement refers to \\'HubSpot,\\' which is an external tool not directly related to the core components of MCP as per the context provided.\"\\n            },\\n            {\\n                \"statement\": \"Security considerations for MCP serversMCP\\\\u2019s OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement is more about security aspects and specific technical details, which are not directly related to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Let\\\\u2019s take a closer look at what happens when a user asks Claude Desktop to perform a task that invokes tools outside the chat window.Protocol handshakeInitial connection: When an MCP client (like Claude Desktop) starts up, it connects to the configured MCP servers on your device.Capability discovery: The client asks each server \\'What capabilities do you\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement is a detailed explanation of the protocol handshake and capability discovery process, which is more technical than the core components.\"\\n            },\\n            {\\n                \"statement\": \"problemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.Subscribe\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement contains multiple topics, some unrelated to the core components of MCP.\"\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What are the core components of MCP, just give the heading', actual_output='MCP Client & Server Ecosystem', expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=['understand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\nsupports MCP, several processes occur behind the scenes to enable quick and seamless communication between the AI and external systems. Let‚Äôs take a closer look at what happens when a user asks Claude Desktop to perform a task that invokes tools outside the chat window.Protocol handshakeInitial connection: When an MCP client (like Claude Desktop) starts up, it connects to the configured MCP servers on your device.Capability discovery: The client asks each server \"What capabilities do you\\n\\nproblemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But']), TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=0.8, reason='The score is 0.80 because the actual output contained an irrelevant statement that does not address the question about MCP.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"MCP stands for Model Context Protocol.\",\\n    \"It\\'s a protocol designed to enable AI assistants to interact with external APIs.\",\\n    \"It retrieves information from various sources like popular messaging apps and GitHub repositories.\",\\n    \"The protocol supports actions such as creating forks or branches, listing issues, making pull requests, and searching for code across GitHub repositories.\",\\n    \"MCP servers can be reference, official integrations by companies, or community-developed.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is unrelated to the original input about MCP.\"\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions found between the actual output and the retrieval context.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n    \"GitHub provides support for creating forks or branches and listing issues among other actions.\"\\n] \\n \\nClaims:\\n[\\n    \"MCP stands for Model Context Protocol.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because all relevant nodes are ranked higher than irrelevant ones.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"It explains what MCP is and how it works, directly answering the input query.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"There\\'s no mention of any specific channel or history retrieval as mentioned in the context.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.5714285714285714, reason='The score is 0.57 because the retrieval context is too generic and does not provide specific information about what MCP is.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GitHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Official MCP\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Official MCP\\' is too vague and does not provide specific information related to the input query about what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"LangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Examples of MCP servers\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Examples of MCP servers\\' is too generic and does not provide specific information related to the input query about what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"Reference servers demonstrate core MCP\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"What Is the Model Context Protocol (MCP) and How It Works\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'What Is the Model Context Protocol (MCP) and How It Works\\' is the main title of an article or document, which is too general and does not directly answer what MCP is.\"\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What is MCP', actual_output=\"MCP stands for Model Context Protocol. It's a protocol designed to enable AI assistants to interact with external APIs and retrieve information from various sources, including popular messaging apps and GitHub repositories. The protocol supports a wide range of actions such as creating forks or branches, listing issues, making pull requests, and searching for code across GitHub repositories. MCP servers can be reference, official integrations by companies, or community-developed, demonstrating its versatility within the AI assistant ecosystem.\", expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=[\"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\\n\\nLinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\"]), TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the provided answer did not address the relationship between function calling and Model Context Protocol, focusing instead on unrelated topics like the importance of understanding relationships in general.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"The relationship between function calling and the Model Context Protocol lies in their roles within AI application development.\",\\n    \"Function calling allows large language models to invoke predetermined functions based on user requests.\",\\n    \"MCP builds upon function calling by providing a more structured way for AI applications to access context and interact with external APIs.\",\\n    \"MCP aims to simplify and standardize API interaction, making development simpler and more consistent across different applications.\",\\n    \"While function calling enables LLMs to perform specific actions based on user input, MCP enhances this by providing a framework that allows AI apps to understand and utilize context more effectively.\",\\n    \"MCP can be seen as an extension or improvement over traditional function calling methods, especially when it comes to dealing with the complexity of integrating multiple APIs and managing context across different applications.\",\\n    \"For example, MCP can help in retrieving information from various sources by standardizing how these interactions are handled.\",\\n    \"Providing a more seamless experience for developers building AI applications is one of the key benefits of MCP.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Directly discusses the relationship between function calling and MCP, which addresses the input.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Explains how function calling works in AI applications, relevant to understanding MCP\\'s role.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Describes how MCP builds upon function calling, directly addressing the relationship.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Mentions simplification and standardization of API interactions, which is part of MCP\\'s role relative to function calling.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Compares MCP to traditional methods, showing their relationship in AI app development.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Gives an example of MCP\\'s use, relevant to its function relative to function calling.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Focuses on the developer experience, which is part of MCP\\'s role in relation to function calling.\"\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions found between the actual output and the retrieval context.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP uses a client-server architecture inspired by the Language Server Protocol (LSP).\",\\n    \"The primary function calling method in MCP is based on predetermined functions.\",\\n    \"GitHub provides support for a wide variety of actions through its MCP server.\",\\n    \"MCP connects AI applications with external APIs and context retrieval capabilities.\",\\n    \"MCP allows for integration with various sources such as popular messaging apps.\"\\n] \\n \\nClaims:\\n[\\n    \"Function calling allows large language models to invoke predetermined functions based on user requests.\",\\n    \"The Model Context Protocol builds upon function calling by providing a structured and standardized way for AI applications to access context and interact with external APIs.\",\\n    \"MCP aims to simplify and standardize API interaction, making development simpler and more consistent across different applications regardless of the underlying model vendor.\",\\n    \"While function calling enables LLMs to perform specific actions based on user input, MCP enhances this by providing a framework that allows AI apps to understand and utilize context more effectively.\",\\n    \"MCP can be seen as an extension or improvement over traditional function calling methods, especially when it comes to dealing with the complexity of integrating multiple APIs and managing context across different applications.\",\\n    \"For example, MCP can help in retrieving information from various sources (like messaging apps or GitHub repositories) by standardizing how these interactions are handled, thus providing a more seamless experience for developers building AI applications.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=0.9266666666666665, reason='The score is 0.93 because despite several relevant nodes ranking higher (nodes 1-5), one node (node 6) fails to discuss the relationship directly, making some relevance missing.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"It directly addresses the relationship between function calling and Model Context Protocol, stating that \\'Function calling is a well-established feature...\\'.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Mentions that MCP builds on top of function calling, which connects AI applications to context through predetermined functions.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Describes how MCP uses a client-server architecture inspired by LSP, relating it to function calling in the context of AI model interactions.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This part mentions GitHub integration but does not directly discuss the relationship between function calling and MCP.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Explains that MCP doesn\\'t solve the NxM problem by replacing previous integration methods but instead builds on function calling for consistent API interactions.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"Mentions Superinterface as a platform that helps developers build in-app AI assistants using MCP functionality, which relates to function calling.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"Although it talks about reference servers and community servers, the exact relationship isn\\'t clearly explained here.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.8888888888888888, reason='The score is 0.89 because the retrieval context provides specific details about the Model Context Protocol, including its architecture and capabilities related to function calling and integration with various tools, which are relevant to understanding the relationship between function calling and MCP.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"The Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Official MCP\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The term \\'Official MCP\\' is mentioned, but the context only describes it briefly and doesn\\'t provide specific information about its capabilities or relationship to function calling beyond what\\'s already stated.\"\\n            },\\n            {\\n                \"statement\": \"understand that MCP doesn\\\\u2019t solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling\\\\u2014the primary method for calling APIs from LLMs\\\\u2014to make development simpler and more consistent.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Sometimes function calling is used in conjunction with LangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Examples of MCP servers include reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Reference servers demonstrate core MCP functionality in a minimal way.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What is Relationship between function calling & Model Context Protocol', actual_output=\"The relationship between function calling and the Model Context Protocol (MCP) lies in their roles within AI application development, particularly when it comes to integrating external APIs.\\n\\nFunction calling allows large language models (LLMs) to invoke predetermined functions based on user requests. This is a well-established feature that facilitates the execution of specific tasks or operations as directed by the user's input.\\n\\nThe Model Context Protocol builds upon this concept by providing a more structured and standardized way for AI applications to access context and interact with external APIs. It aims to simplify and standardize API interaction, making development simpler and more consistent across different applications regardless of the underlying model vendor.\\n\\nIn essence, while function calling enables LLMs to perform specific actions based on user input, MCP enhances this by providing a framework that allows AI apps to understand and utilize context more effectively. MCP can be seen as an extension or improvement over traditional function calling methods, especially when it comes to dealing with the complexity of integrating multiple APIs and managing context across different applications.\\n\\nFor example, MCP can help in retrieving information from various sources (like messaging apps or GitHub repositories) by standardizing how these interactions are handled, thus providing a more seamless experience for developers building AI applications.\", expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=['then make the API call with it. While this allows some purpose-built tooling, it‚Äôs bound to OpenAI‚Äôs ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools. Similarly, the aim of\\n\\nreactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nunderstand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP'])], confident_link='https://app.confident-ai.com/project/cm8dzjvmw19bi7x4v8swskpku/evaluation/test-runs/cm8i892bl30xb2oxzb77qhdxu/test-cases')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "\n",
    "deepeval.evaluate(\n",
    "    data, \n",
    "    metrics= [\n",
    "        deepeval.metrics.AnswerRelevancyMetric(),\n",
    "        deepeval.metrics.FaithfulnessMetric(),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(),\n",
    "        deepeval.metrics.ContextualRelevancyMetric()\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
