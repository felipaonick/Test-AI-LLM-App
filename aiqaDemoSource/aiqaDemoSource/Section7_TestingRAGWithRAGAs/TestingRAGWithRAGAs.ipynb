{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications üìë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_42468/1491807655.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps, creating forks or branches on GitHub, and more. MCP servers demonstrate how AI assistants can access diverse information sources, and the ecosystem includes reference, official integrations, and community servers, showcasing different implementations of the protocol.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is MCP\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_context(question):\n",
    "    retrieved_document = retrieve_and_format(question)\n",
    "    response = qa_chain.run(question)\n",
    "    return response, retrieved_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_42468/1524904417.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('MCP stands for Model Context Protocol. It is a protocol designed to facilitate interactions between AI assistants and various external services or platforms, including popular messaging apps and GitHub. This protocol supports a wide range of actions such as retrieving channel history from messaging apps and performing operations on GitHub like creating forks or branches and making pull requests. The MCP ecosystem includes reference servers created by the protocol maintainers, official integrations maintained by companies for their platforms, and community-developed servers.',\n",
       " \"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\\n\\nLinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual, context = query_with_context(\"What is MCP\")\n",
    "\n",
    "actual, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Application with RAGAs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"reference\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'What is MCP',\n",
       "  'retrieved_contexts': [\"reactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin our next live demo to see how to build passwordless user journeys in minutes. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works March 11, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM\\n\\nLinksSSOOpenID ConnectnOTPOne-Time PasswordsAuthenticator AppsPasswordsDevelopersDocsTutorialsCommunityOpen SourceResourcesLearning CenterBlogCompanyOur StoryCareersPartnersNewsroomSecurity & ComplianceContact UsLegalPrivacy PolicyTerms of UseCopyright ¬© Descope Inc. All rights reserved.All systems operationalGithub Icon GreyLinkedin Icon GreyX Grey IconInstagram Grey LogoSlack Grey IconYoutube Grey IconBluesky SocialDescope - Go to homepageChat with SalesAnonymously - no Slack account\"],\n",
       "  'response': 'MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps and performing operations on GitHub such as creating forks or branches and making pull requests. The protocol also includes reference servers provided by the protocol maintainers, official integrations by companies for their platforms, and community-developed servers. MCP serves as a benchmark for building in-app AI assistants with similar functionalities.',\n",
       "  'reference': 'The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.'},\n",
       " {'user_input': 'What is Relationship between function calling & Model Context Protocol',\n",
       "  'retrieved_contexts': ['then make the API call with it. While this allows some purpose-built tooling, it‚Äôs bound to OpenAI‚Äôs ecosystem. MCP brings similar capabilities to any AI application that implements the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools. Similarly, the aim of\\n\\nreactions, retrieve channel history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Provides support for a wide variety of actions, including creating forks or branches, listing issues, making pull requests, and even searching for code across GItHub repositories. The GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.Official MCP\\n\\nunderstand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\nLangChain adapters, and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP'],\n",
       "  'response': 'Function calling, which allows Large Language Models (LLMs) to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. The Model Context Protocol builds upon this concept by connecting AI applications to context while leveraging function calling as the primary method for calling APIs from LLMs. This approach aims to make development simpler and more consistent.\\n\\nBy integrating function calling with the Model Context Protocol, developers can create more interactive and context-aware AI applications without having to worry about the underlying API integration methods. This relationship simplifies the process of building applications that can fetch data, perform actions, or interact with various services seamlessly.',\n",
       "  'reference': 'The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.'},\n",
       " {'user_input': 'What are the core components of MCP, just give the heading',\n",
       "  'retrieved_contexts': ['understand that MCP doesn‚Äôt solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling‚Äîthe primary method for calling APIs from LLMs‚Äîto make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\nsupports MCP, several processes occur behind the scenes to enable quick and seamless communication between the AI and external systems. Let‚Äôs take a closer look at what happens when a user asks Claude Desktop to perform a task that invokes tools outside the chat window.Protocol handshakeInitial connection: When an MCP client (like Claude Desktop) starts up, it connects to the configured MCP servers on your device.Capability discovery: The client asks each server \"What capabilities do you\\n\\nproblemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But'],\n",
       "  'response': 'MCP Client & Server Ecosystem',\n",
       "  'reference': ' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                '}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for question in test_data:\n",
    "    actual, context = query_with_context(question['input'])\n",
    "    \n",
    "    dataset.append({\n",
    "        \"user_input\": question['input'],\n",
    "        \"retrieved_contexts\": [context],\n",
    "        \"response\": actual,\n",
    "        \"reference\": question['reference']\n",
    "    })\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [02:37<00:00, 13.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, NoiseSensitivity, Faithfulness, FactualCorrectness, AnswerRelevancy\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas import (EvaluationDataset, evaluate)\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "result = evaluate(dataset=evaluation_dataset, \n",
    "                  metrics=[LLMContextRecall(),\n",
    "                           Faithfulness(),\n",
    "                           AnswerRelevancy(),\n",
    "                           FactualCorrectness()],\n",
    "                  llm = evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MCP</td>\n",
       "      <td>[reactions, retrieve channel history, and more...</td>\n",
       "      <td>MCP, or Model Context Protocol, is a protocol ...</td>\n",
       "      <td>The Model Context Protocol (MCP) addresses thi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.849991</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Relationship between function calling ...</td>\n",
       "      <td>[then make the API call with it. While this al...</td>\n",
       "      <td>Function calling, which allows Large Language ...</td>\n",
       "      <td>The Model Context Protocol (MCP) builds on top...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832278</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the core components of MCP, just give...</td>\n",
       "      <td>[understand that MCP doesn‚Äôt solve the NxM pro...</td>\n",
       "      <td>MCP Client &amp; Server Ecosystem</td>\n",
       "      <td>\\n                    - MCP Client\\n         ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833868</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                                        What is MCP   \n",
       "1  What is Relationship between function calling ...   \n",
       "2  What are the core components of MCP, just give...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [reactions, retrieve channel history, and more...   \n",
       "1  [then make the API call with it. While this al...   \n",
       "2  [understand that MCP doesn‚Äôt solve the NxM pro...   \n",
       "\n",
       "                                            response  \\\n",
       "0  MCP, or Model Context Protocol, is a protocol ...   \n",
       "1  Function calling, which allows Large Language ...   \n",
       "2                      MCP Client & Server Ecosystem   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  The Model Context Protocol (MCP) addresses thi...             0.0   \n",
       "1  The Model Context Protocol (MCP) builds on top...             1.0   \n",
       "2   \\n                    - MCP Client\\n         ...             1.0   \n",
       "\n",
       "   faithfulness  answer_relevancy  factual_correctness(mode=f1)  \n",
       "0           1.0          0.849991                          0.17  \n",
       "1           1.0          0.832278                          1.00  \n",
       "2           1.0          0.833868                          0.50  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
