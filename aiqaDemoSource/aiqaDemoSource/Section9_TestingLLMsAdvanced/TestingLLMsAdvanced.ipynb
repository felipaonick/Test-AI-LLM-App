{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LLM using HF Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "references = [1,0,1,1]\n",
    "predications = [1,0,0,0]\n",
    "\n",
    "results = accuracy.compute(references=references, predictions=predications)\n",
    "\n",
    "print(f\"Accuracy: {results['accuracy']:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision (Exact Match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "references = [\"Execute\", \"Automation\"]\n",
    "predications = [\"Execute\", \"Automations\"]\n",
    "\n",
    "results = exact_match.compute(references=references, predictions=predications)\n",
    "\n",
    "print(f\"Exact Match: {results['exact_match']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 76.92%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "#True label/Ground truth/reference\n",
    "references = [1,0,1,1,0,1,0,1,1]\n",
    "predications = [1,0,1,1,1,1,1,0,1]\n",
    "\n",
    "results = f1.compute(references=references, predictions=predications, average=\"binary\")\n",
    "\n",
    "print(f\"F1-Score: {results['f1']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HF Evaluate Method to Evalute a LLM using Pipeline function ü§ñ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "datasets = [\n",
    "    {\"text\": \"I love learning ML and AI in Testing\", \"label\": 0},\n",
    "    {\"text\": \"I hate working with a machine which has got no GPU for my AI training\", \"label\": 0},\n",
    "    {\"text\": \"I like driving fast cars\", \"label\": 1}\n",
    "]\n",
    "\n",
    "predications = sentiment_pipeline([data[\"text\"] for data in datasets])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9988466501235962},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995275735855103},\n",
       " {'label': 'POSITIVE', 'score': 0.9991025924682617}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 1], [0, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predications]\n",
    "true_labels = [dataset['label'] for dataset in datasets]\n",
    "\n",
    "prediction_labels, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "results = accuracy.compute(predictions=prediction_labels, references=true_labels)\n",
    "\n",
    "print(f\"Accuracy: {results['accuracy']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6667\n",
      "Precision: 0.5000\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "accuracy_score = accuracy.compute(predictions=prediction_labels, references=true_labels)\n",
    "precision_score = precision.compute(predictions=prediction_labels, references=true_labels)\n",
    "recall_score = recall.compute(predictions=prediction_labels, references=true_labels)\n",
    "f1_score = f1.compute(predictions=prediction_labels, references=true_labels)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score['accuracy']:.4f}\")\n",
    "print(f\"Precision: {precision_score['precision']:.4f}\")\n",
    "print(f\"Recall: {recall_score['recall']:.4f}\")\n",
    "print(f\"F1 Score: {f1_score['f1']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
